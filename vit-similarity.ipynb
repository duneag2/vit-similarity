{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri July 29 2022\n",
    "Last revised on Sun June 16 2023\n",
    "\n",
    "A Jupyer Notebook for Multi-input Vision Transformer with Similarity Matching\n",
    "\n",
    "@author: Anonymous\n",
    "\n",
    "Multi-input Vision Transformer with Similarity Matching\n",
    "1) Backbone network: ViT-L/32, ResNet50\n",
    "2) Cosine similarity\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "import os.path\n",
    "from glob import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pydicom\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "import seaborn as sns\n",
    "import timm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zShuPxbwegA-",
    "outputId": "da24ec12-4b79-417a-8fb4-eb80ae809363"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(256)\n",
    "torch.cuda.manual_seed(256)\n",
    "torch.cuda.manual_seed_all(256)\n",
    "np.random.seed(0)\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "code1 = []\n",
    "cropped1 = []\n",
    "\n",
    "img_path = 'path/to/class0'\n",
    "\n",
    "for i in range(len(os.listdir(img_path))):\n",
    "    temp = os.listdir(img_path)\n",
    "    path = glob(os.path.join(img_path, temp[i]))\n",
    "    temp = pydicom.read_file(path)\n",
    "    # original image size: 384*384 for ViT-L/32\n",
    "    img = temp.pixel_array\n",
    "    \n",
    "    ds = pydicom.dcmread(path)\n",
    "    imgnum = ds[0x0028,0x0004].value\n",
    "    print(imgnum.lower())\n",
    "    if imgnum.lower() == 'monochrome1':\n",
    "        img = np.invert(img)\n",
    "        print('converted')\n",
    "    \n",
    "    img2 = cv2.resize(img, (384, 384), interpolation = cv2.INTER_LANCZOS4)\n",
    "    scaler = StandardScaler()\n",
    "    scaled = scaler.fit_transform(img2)\n",
    "    code1.append(torch.stack([torch.tensor(scaled), torch.tensor(scaled), torch.tensor(scaled)], dim=0))\n",
    "    \n",
    "    # ROI-cropped image\n",
    "    img3 = cv2.resize(img, (2048, 2048), interpolation = cv2.INTER_LANCZOS4)\n",
    "    img4 = img3[200:2000, 200:2000]\n",
    "    scaler = StandardScaler()\n",
    "    img5 = scaler.fit_transform(img4)\n",
    "    img5 = cv2.resize(img5, (384, 384), interpolation = cv2.INTER_LANCZOS4)\n",
    "    cropped1.append(torch.stack([torch.tensor(img5), torch.tensor(img5), torch.tensor(img5)], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2 = []\n",
    "cropped2 = []\n",
    "\n",
    "img_path = 'path/to/class1'\n",
    "\n",
    "for i in range(len(os.listdir(img_path))):\n",
    "    temp = os.listdir(img_path)\n",
    "    path = glob(os.path.join(img_path, temp[i]))\n",
    "    temp = pydicom.read_file(path)\n",
    "    # original image size: 384*384 for ViT-L/32\n",
    "    img = temp.pixel_array\n",
    "    \n",
    "    ds = pydicom.dcmread(path)\n",
    "    imgnum = ds[0x0028,0x0004].value\n",
    "    print(imgnum.lower())\n",
    "    if imgnum.lower() == 'monochrome1':\n",
    "        img = np.invert(img)\n",
    "        print('converted')\n",
    "    \n",
    "    img2 = cv2.resize(img, (384, 384), interpolation = cv2.INTER_LANCZOS4)\n",
    "    scaler = StandardScaler()\n",
    "    scaled = scaler.fit_transform(img2)\n",
    "    code2.append(torch.stack([torch.tensor(scaled), torch.tensor(scaled), torch.tensor(scaled)], dim=0))\n",
    "    \n",
    "    # ROI-cropped image\n",
    "    img3 = cv2.resize(img, (2048, 2048), interpolation = cv2.INTER_LANCZOS4)\n",
    "    img4 = img3[200:2000, 200:2000]\n",
    "    scaler = StandardScaler()\n",
    "    img5 = scaler.fit_transform(img4)\n",
    "    img5 = cv2.resize(img5, (384, 384), interpolation = cv2.INTER_LANCZOS4)\n",
    "    cropped2.append(torch.stack([torch.tensor(img5), torch.tensor(img5), torch.tensor(img5)], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limited = code1\n",
    "good = code2\n",
    "limited_c = cropped1\n",
    "good_c = cropped2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "\n",
    "for i in range(len(limited)):\n",
    "    target.append(0)\n",
    "    \n",
    "for i in range(len(good)):\n",
    "    target.append(1)\n",
    "\n",
    "idx_list = []\n",
    "for i in range(len(target)):\n",
    "  idx_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train, idx_test = train_test_split(idx_list, test_size = 0.1, shuffle = True, random_state = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = limited + good\n",
    "data_c = limited_c + good_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = []\n",
    "trainidx = []\n",
    "traindata_c = []\n",
    "\n",
    "for i in range(len(idx_train)):\n",
    "    traindata.append(data[idx_train[i]])\n",
    "    trainidx.append(target[idx_train[i]])\n",
    "    traindata_c.append(data_c[idx_train[i]])\n",
    "\n",
    "testdata = []\n",
    "testidx = []\n",
    "testdata_c = []\n",
    "\n",
    "for i in range(len(idx_test)):\n",
    "    testdata.append(data[idx_test[i]])\n",
    "    testidx.append(target[idx_test[i]])\n",
    "    testdata_c.append(data_c[idx_test[i]])\n",
    "\n",
    "trainidx = np.array(trainidx)\n",
    "testidx = np.array(testidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    a_norm = torch.linalg.norm(a)\n",
    "    b_norm = torch.linalg.norm(b)\n",
    "    a_b_dot = torch.inner(a, b)\n",
    "    return torch.mean(a_b_dot / (a_norm * b_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_norm(a, b):\n",
    "    add_inv_b = torch.mul(b, -1)\n",
    "    summation = torch.add(a, add_inv_b)\n",
    "    abs_val = torch.abs(summation)\n",
    "    return torch.sum(abs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_norm(a, b):\n",
    "    add_inv_b = torch.mul(b, -1)\n",
    "    summation = torch.add(a, add_inv_b)\n",
    "    square = torch.mul(summation, summation)\n",
    "    sqrt = torch.sqrt(square)\n",
    "    return torch.sum(sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "USvWqMi_odBB",
    "outputId": "e9095b25-f740-438c-dfeb-2af52635a6b1"
   },
   "outputs": [],
   "source": [
    "model1 = timm.create_model('vit_large_patch32_384', pretrained=True).cuda()\n",
    "\n",
    "\n",
    "for parameter in model1.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "\n",
    "num_features = model1.head.in_features\n",
    "\n",
    "model1.head = nn.Sequential(\n",
    "    nn.Linear(num_features, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model1 = model1.cuda()\n",
    "\n",
    "model2 = timm.create_model('vit_large_patch32_384', pretrained=True).cuda()\n",
    "\n",
    "\n",
    "for parameter in model2.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "\n",
    "num_features = model2.head.in_features\n",
    "\n",
    "model2.head = nn.Sequential(\n",
    "    nn.Linear(num_features, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model2 = model2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoInputsNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TwoInputsNet, self).__init__()\n",
    "        # model1, model2: pre-trained ViT-L/32\n",
    "        self.model1 = torch.nn.Sequential(*list(model1.children())[:-2])\n",
    "        self.model2 = torch.nn.Sequential(*list(model2.children())[:-2])\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(294912, 1000),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1000, 100),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(100, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        c = self.model1(input1)\n",
    "        f = self.model2(input2)\n",
    "        combined = torch.cat([c, f], dim = 2)\n",
    "        combined2 = combined.reshape(c.shape[0], -1)\n",
    "        out = self.fc(combined2)\n",
    "        return out\n",
    "\n",
    "model_merged = TwoInputsNet().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 64\n",
    "optimizer = torch.optim.Adam(model_merged.parameters(), lr = 0.001)\n",
    "criterion = nn.BCELoss()\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch : 0.95 ** epoch)\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfinal = []\n",
    "for i in range(len(testdata)):\n",
    "    testfinal.append((testdata[i], testdata_c[i], testidx[i]))\n",
    "\n",
    "testloader = DataLoader(testfinal, batch_size = batchsize, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sn89Cgr0UsB3"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8lgmhGrVMOK"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (image1, image2, label) in enumerate(train_loader):\n",
    "        image1 = image1.to(device, dtype=torch.float)\n",
    "        image2 = image2.to(device, dtype=torch.float)\n",
    "        label = label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image1, image2).squeeze(dim=1)\n",
    "        loss = criterion(output.to(torch.float32), label.to(torch.float32))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "          print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(epoch, batch_idx * len(image1), \n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    testlabel = []\n",
    "    testpred = []\n",
    "    testprob = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image1, image2, label in test_loader:\n",
    "            image1 = image1.to(device, dtype = torch.float)\n",
    "            image2 = image2.to(device, dtype = torch.float)\n",
    "            label = label.to(device)\n",
    "            testlabel.append(label)\n",
    "            output = model(image1, image2).squeeze(dim=1)\n",
    "            testprob.append(output.to(torch.float32))\n",
    "            test_loss += criterion(output.to(torch.float32), label.to(torch.float32)).item()\n",
    "            output1 = output.cpu()\n",
    "            output1[output1 >= 0.5] = 1\n",
    "            output1[output1 < 0.5] = 0\n",
    "            correct += output1.eq(label.cpu()).int().sum()\n",
    "            testpred.append(output1)\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy, testlabel, testpred, testprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = 10\n",
    "skf = StratifiedKFold(n_splits = kfold, shuffle = True, random_state = 256)\n",
    "traindata1 = np.array(traindata)\n",
    "traindata2 = np.array(traindata_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlabel = []\n",
    "testpred = []\n",
    "testloss = []\n",
    "testacc = []\n",
    "testprob = []\n",
    "\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(skf.split(traindata1, trainidx)):\n",
    "    \n",
    "    \n",
    "    print('[Fold %d/%d]' % (i + 1, kfold))\n",
    "    \n",
    "    X_train, X_valid = traindata1[train_index], traindata1[val_index]\n",
    "    y_train, y_valid = trainidx[train_index], trainidx[val_index]\n",
    "    \n",
    "    # Split the original and ROI-cropped images with respect to their classes - the given classes of input pairs aren't mixed up.\n",
    "    \n",
    "    original_0 = []\n",
    "    original_1 = []\n",
    "    \n",
    "    for j in range(len(y_train)):\n",
    "        if y_train[j] == 0:\n",
    "            original_0.append(traindata1[train_index[j]])\n",
    "        else:\n",
    "            original_1.append(traindata1[train_index[j]])\n",
    "    \n",
    "    cropped_0 = []\n",
    "    cropped_1 = []\n",
    "    \n",
    "    for k in range(len(y_train)):\n",
    "        if y_train[k] == 0:\n",
    "            cropped_0.append(traindata2[train_index[k]])\n",
    "        else:\n",
    "            cropped_1.append(traindata2[train_index[k]])\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "\n",
    "    metric_0 = []\n",
    "    metric_1 = []\n",
    "    \n",
    "    for a in range(len(original_0)):\n",
    "        for b in range(len(cropped_0)):\n",
    "            metric_0.append((cosine_similarity(original_0[a][0], cropped_0[b][0])).detach().cpu().numpy().item())\n",
    "\n",
    "    for c in range(len(original_1)):\n",
    "        for d in range(len(cropped_1)):\n",
    "            metric_1.append((cosine_similarity(original_1[c][0], cropped_1[d][0])).detach().cpu().numpy().item())\n",
    "    \n",
    "    # List chunk (List comprehension)\n",
    "    \n",
    "    metric_0_chunk = [metric_0[e * len(cropped_0):(e + 1) * len(cropped_0)] for e in range((len(metric_0) + len(cropped_0) - 1) // len(cropped_0) )]\n",
    "    metric_1_chunk = [metric_1[f * len(cropped_1):(f + 1) * len(cropped_1)] for f in range((len(metric_1) + len(cropped_1) - 1) // len(cropped_1) )]\n",
    "    \n",
    "    # Find the index having the lowest cosine similarity\n",
    "    \n",
    "    min_0 = []\n",
    "    min_1 = []\n",
    "    \n",
    "    for g in range(len(metric_0_chunk)):\n",
    "        min_0.append(np.argmin(metric_0_chunk[g]))\n",
    "        \n",
    "        # Removing duplicates process\n",
    "        for n in range(len(metric_0_chunk)):\n",
    "            metric_0_chunk[n][min_0[g]] = np.inf        \n",
    "\n",
    "    for h in range(len(metric_1_chunk)):\n",
    "        min_1.append(np.argmin(metric_1_chunk[h]))\n",
    "        \n",
    "        # Removing duplicates process\n",
    "        for o in range(len(metric_1_chunk)):\n",
    "            metric_1_chunk[o][min_1[h]] = np.inf  \n",
    "    \n",
    "    # Aggregate two matched images\n",
    "\n",
    "    cropped_0_sorted = []\n",
    "    cropped_1_sorted = []\n",
    "    \n",
    "    for l in range(len(min_0)):\n",
    "        cropped_0_sorted.append(cropped_0[min_0[l]])\n",
    "\n",
    "    for m in range(len(min_1)):\n",
    "        cropped_1_sorted.append(cropped_1[min_1[m]])\n",
    "\n",
    "    new_traindata2 = []\n",
    "    \n",
    "    cropped_num0 = 0\n",
    "    cropped_num1 = 0\n",
    "    \n",
    "    for k in range(len(y_train)):\n",
    "        if y_train[k] == 0:\n",
    "            new_traindata2.append(cropped_0_sorted[cropped_num0])\n",
    "            cropped_num0 += 1\n",
    "\n",
    "        else:\n",
    "            new_traindata2.append(cropped_1_sorted[cropped_num1])\n",
    "            cropped_num1 += 1\n",
    "    \n",
    "    new_traindata2 = np.array(new_traindata2)\n",
    "    \n",
    "    # There is no need to implement Similarity Matching for validation and test process. \n",
    "    X_train2, X_valid2 = new_traindata2, traindata2[val_index]\n",
    "    \n",
    "    \n",
    "    trainfinal = []\n",
    "\n",
    "    for h in range(X_train.shape[0]):\n",
    "      trainfinal.append((X_train[h], X_train2[h], y_train[h]))\n",
    "\n",
    "    valfinal = []\n",
    "\n",
    "    for t in range(X_valid.shape[0]):\n",
    "      valfinal.append((X_valid[t], X_valid2[t], y_valid[t]))\n",
    "\n",
    "    trainloader = DataLoader(trainfinal, batch_size = batchsize, shuffle = False)\n",
    "    validloader = DataLoader(valfinal, batch_size = batchsize, shuffle = False)\n",
    "    \n",
    "    print('[Fold %d/%d Prediciton:]' % (i + 1, kfold))\n",
    "    \n",
    "\n",
    "\n",
    "    # Train and Validation Process\n",
    "    epochval = []\n",
    "    valloss = []\n",
    "    valacc = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model_merged, trainloader, optimizer, log_interval = 5)\n",
    "        valid_loss, valid_accuracy, _, _, _ = evaluate(model_merged, validloader)\n",
    "        epochval.append(epoch)\n",
    "        valloss.append(valid_loss)\n",
    "        valacc.append(valid_accuracy)\n",
    "        print(\"\\n[EPOCH: {}], \\tValidation Loss: {:.6f}, \\tValidation Accuracy: {:.6f} % \\n\".format(\n",
    "            epoch, valid_loss, valid_accuracy))\n",
    "    \n",
    "\n",
    "    # Test Process\n",
    "    test_loss, test_accuracy, label, pred, prob = evaluate(model_merged, testloader)\n",
    "    testlabel.append(label)\n",
    "    testpred.append(pred)\n",
    "    testloss.append(test_loss)\n",
    "    testacc.append(test_accuracy)\n",
    "    testprob.append(prob)\n",
    "\n",
    "    print(\"\\nTest Loss: {:.4f}, \\tTest Accuracy: {:.4f} % \\n\".format(test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether every entry of metric_0_chunk is inf \n",
    "print(metric_0_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_list = []\n",
    "for i in range(len(testprob[1])):\n",
    "    num_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve_fold_red(fper, tper):\n",
    "    sns.lineplot(x = fper, y = tper, ci=None, color='red', alpha = 0.08)\n",
    "\n",
    "def plot_roc_curve_red(fper, tper):\n",
    "    sns.lineplot(x = fper, y = tper, ci=None, color='red', alpha = 1)\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    \n",
    "def plot_roc_curve_fold_blue(fper, tper):\n",
    "    sns.lineplot(x = fper, y = tper, ci=None, color='blue', alpha = 0.08)\n",
    "\n",
    "def plot_roc_curve_blue(fper, tper):\n",
    "    sns.lineplot(x = fper, y = tper, ci=None, color='blue', alpha = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 8))\n",
    "auc10 = []\n",
    "temptpr = []\n",
    "tempfpr = []\n",
    "    \n",
    "for i in range(kfold):\n",
    "    true = np.concatenate([testlabel[i][j].detach().cpu().numpy() for j in num_list])\n",
    "    prob = np.concatenate([testprob[i][j].detach().cpu().numpy() for j in num_list])\n",
    "    fpr, tpr, _ = roc_curve(true, prob)\n",
    "    \n",
    "    tempfpr.append(fpr)\n",
    "    temptpr.append(tpr)\n",
    "    \n",
    "    plot_roc_curve_fold_red(fpr, tpr)\n",
    "    currauc = auc(fpr, tpr)\n",
    "    \n",
    "    auc10.append(currauc)\n",
    "    \n",
    "plot_roc_curve_red(np.sort(np.array(pd.DataFrame(tempfpr).mean())), np.sort(np.array(pd.DataFrame(temptpr).mean())))\n",
    "ax = plt.gca()\n",
    "ax.axes.xaxis.set_visible(False)\n",
    "ax.axes.yaxis.set_visible(False)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.legend(labels = ['ViT-L/32'], loc='lower right', fontsize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens = []\n",
    "spec = []\n",
    "ppv_list = []\n",
    "npv_list = []\n",
    "\n",
    "for i in range(kfold):\n",
    "    tn, fp, fn, tp = confusion_matrix(np.concatenate([testlabel[i][j].detach().cpu().numpy() for j in num_list]).astype(int), np.concatenate([testpred[i][j].detach().cpu().numpy() for j in num_list]).astype(int)).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    ppv = tp / (tp + fp)\n",
    "    npv = tn / (tn + fn)\n",
    "    sens.append(sensitivity)\n",
    "    spec.append(specificity)\n",
    "    ppv_list.append(ppv)\n",
    "    npv_list.append(npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(testacc)/100, np.mean(auc10), np.mean(sens), np.mean(spec), np.mean(ppv_list), np.mean(npv_list))\n",
    "print(np.std(testacc), np.std(auc10), np.std(sens), np.std(spec), np.std(ppv_list), np.std(npv_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_merged, 'vit-similarity.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dogs-vs-cats-demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1cd114e494fb40eebd4e4705b4a26fc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a55c89232494460a8d03352b967fc53",
      "placeholder": "​",
      "style": "IPY_MODEL_4ce35a2823f24cb685e221211da8b835",
      "value": " 97.8M/97.8M [00:03&lt;00:00, 32.0MB/s]"
     }
    },
    "212b7267229647e98b5a39bbe8d60fb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "22028345e35943dd88e646cc2232c77f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3f4c8cab81914a31ba8229357886945b",
       "IPY_MODEL_3ed987769cb74844ba22a48de889d845",
       "IPY_MODEL_1cd114e494fb40eebd4e4705b4a26fc6"
      ],
      "layout": "IPY_MODEL_c2c5f8fc0c6a4b33aad38e7ff2beabf7"
     }
    },
    "22746b7a92ae490fb8cd3fc128697a48": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ed987769cb74844ba22a48de889d845": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a37a0c6321844cba5d8b8819c6706fb",
      "max": 102530333,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e4530d13fd4b4bf5a2f97bbf25b6b2b5",
      "value": 102530333
     }
    },
    "3f4c8cab81914a31ba8229357886945b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_22746b7a92ae490fb8cd3fc128697a48",
      "placeholder": "​",
      "style": "IPY_MODEL_212b7267229647e98b5a39bbe8d60fb6",
      "value": "100%"
     }
    },
    "4a37a0c6321844cba5d8b8819c6706fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a55c89232494460a8d03352b967fc53": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ce35a2823f24cb685e221211da8b835": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c2c5f8fc0c6a4b33aad38e7ff2beabf7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4530d13fd4b4bf5a2f97bbf25b6b2b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
